{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michelle/anaconda3/lib/python3.6/site-packages/janitor/dataframe.py:24: UserWarning: Janitor's subclassed DataFrame and Series will be deprecated before\n",
      "the 1.0 release. Instead of importing the Janitor DataFrame, please instead\n",
      "`import janitor`, and use the functions directly attached to native pandas\n",
      "dataframe.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy as sp\n",
    "import feather\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS \n",
    "from collections import Counter\n",
    "from plotnine import *\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urls(base_string, num_urls):\n",
    "    string_list = []\n",
    "    for i in range(1,num_urls):\n",
    "        new_string = base_string + str(i)\n",
    "        string_list.append(new_string)\n",
    "    return string_list\n",
    "\n",
    "def generate_artist_album_data(url_list):\n",
    "\n",
    "    artist_list = []\n",
    "    album_list = []\n",
    "    link_list = []\n",
    "    author_list = []\n",
    "    score_list = []\n",
    "    text_list = []\n",
    "    pub_date = []\n",
    "    \n",
    "    counter = 1\n",
    "    for url in url_list:\n",
    "        print('Retrieving {}. {} of {} retrieved.'.format(url,counter,len(url_list)))\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        artist_info = soup.findAll(['a', 'ul', 'h2'], attrs={'class': ['artist-list review__title-artist']})\n",
    "        album_info = soup.findAll(['a', 'ul', 'h2'], attrs={'class': 'review__title-album' })\n",
    "        link_info = soup.findAll(['a'], attrs={'class': 'review__link'})\n",
    "\n",
    "        for artist in artist_info:\n",
    "            artist_list.append(artist.text)\n",
    "\n",
    "\n",
    "        for album in album_info:\n",
    "            album_list.append(album.text)\n",
    "\n",
    "        for link in link_info:\n",
    "            base_link = 'https://pitchfork.com'\n",
    "            link_list.append(base_link + link['href'])\n",
    "        counter += 1\n",
    "    \n",
    "    return link_list\n",
    "\n",
    "def get_album_data(urls):\n",
    "    \n",
    "    album_df = pd.DataFrame({'publication_date': [], 'author': [], 'artist':[], 'album': [], 'score':[], 'review': []})\n",
    "    \n",
    "    counter = 1\n",
    "    for url in urls:\n",
    "        # Read in HTML from link\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        if soup.findAll(['div'], attrs={'class': ['contents dropcap']}):\n",
    "            review = soup.findAll(['div'], attrs={'class': ['contents dropcap']})\n",
    "\n",
    "        else:\n",
    "            review = soup.findAll(['div'], attrs={'class': ['review-detail__article-content']})\n",
    "        try:\n",
    "            x=soup.findAll(['script'], attrs={'type': [\"text/javascript\"]})\n",
    "            x = x[2].string\n",
    "            data = x.split(\"window.digitalData=\", 1)[1]\n",
    "            data = json.loads(data)\n",
    "\n",
    "            publication_date = pd.to_datetime(data['publishDate'])\n",
    "            author = data['authors']\n",
    "            artist = data['display'].split(':')[0].rstrip()\n",
    "            artist = artist.replace('&amp;', 'and')\n",
    "            artist = artist.replace('&quot;', '')\n",
    "            artist = artist.replace('-&gt;', '')\n",
    "            album = data['display'].split(':')[1].lstrip()\n",
    "            album = album.replace('&amp;', 'and')\n",
    "            album = album.replace('&quot;', '')\n",
    "            album = album.replace('-&gt;', '')\n",
    "\n",
    "            score = soup(text=re.compile('window.App'))[0]\n",
    "            score = score.split(\"window.App=\")[1].rstrip(';')\n",
    "            score = json.loads(score)\n",
    "            score=score['context']['dispatcher']['stores']['ReviewsStore']['items']\n",
    "            key = [i for i in score][0]\n",
    "            score = score[key]['tombstone']['albums'][0]['rating']['rating']\n",
    "\n",
    "            print('Artist: {}, Album: {}'.format(artist,album))\n",
    "\n",
    "            df_to_append = pd.DataFrame({'publication_date':[ publication_date], 'author': [author], 'artist':[artist], 'album': [album], 'score':[score], 'review': [review[0].text]})\n",
    "\n",
    "            album_df = album_df.append(df_to_append, ignore_index=True)\n",
    "            print('{} of {} completed'.format(counter,len(urls)))\n",
    "            counter += 1\n",
    "        except:\n",
    "            print('Could not extract {}'.format(url))\n",
    "            counter +=1\n",
    "    \n",
    "    album_df['score'] = album_df['score'].astype(float)\n",
    "    \n",
    "    return album_df\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    # lemmatize\n",
    "    #tokens = [i.lemma_ for i in sample]\n",
    "    #tokens = [i for i in tokens if i not in stopwords]\n",
    "    #tokens = [i for i in tokens if i != '-PRON-']\n",
    "    #tokens = [i for i in tokens if i.pos_ != 'SYM']\n",
    "    #tokens = [i for i in tokens if i.pos_ != 'PUNCT']\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    tokens = [i.lower_ for i in sample if i.lower_ not in list(stopwords) and i.pos_ != '-PRON-' and i.pos_ != 'SYM' and i.pos_ != \"PUNCT\"]\n",
    "    tokens = [i for i in tokens if i not in ['n’t','\"', ',', ',', ':', '.', '/', '-', '’s', '\\n', '—', '’', '’s']]\n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    #tokens = [i for i in freq_list if i in tokens]\n",
    "    \n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "    \n",
    "    doc = ' '.join(tokens)\n",
    "\n",
    "    return doc\n",
    "\n",
    "def create_corpus(df):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc_list = list(df['review'])\n",
    "    doc_list_new = []\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        try:\n",
    "            doc = nlp(doc)\n",
    "            doc = tokenizeText(doc)\n",
    "            doc_list_new.append(doc)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return doc_list_new\n",
    "\n",
    "def split_corpus(corpus, num_words):\n",
    "    word_list = []\n",
    "    for doc in corpus:\n",
    "        doc = doc.split(' ')\n",
    "        for word in doc:\n",
    "            word_list.append(word)\n",
    "    word_freq = Counter(word_list)\n",
    "    common_words = word_freq.most_common(num_words)\n",
    "    \n",
    "    most_freq_list = []\n",
    "    for i in common_words:\n",
    "        most_freq_list.append(i[0])\n",
    "    \n",
    "    return most_freq_list\n",
    "\n",
    "def get_best_new_music(num_urls):\n",
    "    album_df = pd.DataFrame({'artist':[], 'album': []})\n",
    "\n",
    "    for i in range(1,num_urls): \n",
    "        url = 'https://pitchfork.com/reviews/best/albums/?page=' + str(i)\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        soup = soup.findAll(['script'])\n",
    "        soup=soup[8].string.split(\"window.App=\")[1]\n",
    "        soup = soup.rstrip(';')\n",
    "        soup = json.loads(soup)\n",
    "        data = soup['context']['dispatcher']['stores']['ReviewsStore']['items']\n",
    "        id_list = list(data)\n",
    "        for i in id_list:\n",
    "            #print(i)\n",
    "            if data[i]['tombstone']['albums'][0]['album']['display_name']:\n",
    "                artist = data[i]['tombstone']['albums'][0]['album']['display_name']\n",
    "            else:\n",
    "                artist = data[i]['tombstone']['albums'][0]['album']['artists'][0]['display_name']\n",
    "            \n",
    "            artist = artist.replace('&amp;', 'and')\n",
    "            artist = artist.replace('&quot;', '')\n",
    "            artist = artist.replace('-&gt;', '')\n",
    "            \n",
    "            album = data[i]['tombstone']['albums'][0]['album']['display_name']\n",
    "            album = album.replace('&amp;', 'and')\n",
    "            album = album.replace('&quot;', '')\n",
    "            album = album.replace('-&gt;', '')\n",
    "            \n",
    "            df_to_append = pd.DataFrame({'artist':[artist], 'album': [album]})\n",
    "            album_df = album_df.append(df_to_append,ignore_index=True)\n",
    "            \n",
    "    return album_df\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchfork_data = feather.read_dataframe('pitchfork_reviews.feather')\n",
    "pitchfork_data.rename({'artist': 'artist_name'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(pitchfork_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=TfidfVectorizer()\n",
    "matrix =cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_frequency_list = sorted(zip(cv.get_feature_names(),\n",
    "    np.asarray(matrix.sum(axis=0)).ravel()), key=lambda x: x[1], reverse=True)\n",
    "zipped_frequency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_frequency(zipped_items, top_items):\n",
    "    zip_list = []\n",
    "    counter = 0\n",
    "    for i in zipped_items:\n",
    "        if counter == top_items:\n",
    "            break\n",
    "        else:\n",
    "            print(i[0])\n",
    "            zip_list.append(i[0])\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            \n",
    "    return zip_list\n",
    "vocab_list = filter_by_frequency(zipped_frequency_list, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=TfidfVectorizer(vocabulary=vocab_list)\n",
    "matrix =cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2 = pd.concat([pitchfork_data, pd.DataFrame(matrix.todense(), columns=cv.get_feature_names())], axis=1).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_new = pd.read_csv('best_new_music.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_new['category'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_new.rename({'artist':'artist_name'}, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2=album_df2.merge(best_new, how='left', on='artist_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2['category']=album_df2['category'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2.to_csv('pitchform_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2 = pd.read_csv('pitchform_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2[['category']] = album_df2[['category']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2 = album_df2.clean_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2.drop('album_y', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather.write_dataframe(album_df2, 'pitchfork_tfidf.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
