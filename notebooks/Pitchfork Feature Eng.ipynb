{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michelle/anaconda3/lib/python3.6/site-packages/janitor/dataframe.py:24: UserWarning: Janitor's subclassed DataFrame and Series will be deprecated before\n",
      "the 1.0 release. Instead of importing the Janitor DataFrame, please instead\n",
      "`import janitor`, and use the functions directly attached to native pandas\n",
      "dataframe.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy as sp\n",
    "import feather\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS \n",
    "from collections import Counter\n",
    "from plotnine import *\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urls(base_string, num_urls):\n",
    "    string_list = []\n",
    "    for i in range(1,num_urls):\n",
    "        new_string = base_string + str(i)\n",
    "        string_list.append(new_string)\n",
    "    return string_list\n",
    "\n",
    "def generate_artist_album_data(url_list):\n",
    "\n",
    "    artist_list = []\n",
    "    album_list = []\n",
    "    link_list = []\n",
    "    author_list = []\n",
    "    score_list = []\n",
    "    text_list = []\n",
    "    pub_date = []\n",
    "    \n",
    "    counter = 1\n",
    "    for url in url_list:\n",
    "        print('Retrieving {}. {} of {} retrieved.'.format(url,counter,len(url_list)))\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        artist_info = soup.findAll(['a', 'ul', 'h2'], attrs={'class': ['artist-list review__title-artist']})\n",
    "        album_info = soup.findAll(['a', 'ul', 'h2'], attrs={'class': 'review__title-album' })\n",
    "        link_info = soup.findAll(['a'], attrs={'class': 'review__link'})\n",
    "\n",
    "        for artist in artist_info:\n",
    "            artist_list.append(artist.text)\n",
    "\n",
    "\n",
    "        for album in album_info:\n",
    "            album_list.append(album.text)\n",
    "\n",
    "        for link in link_info:\n",
    "            base_link = 'https://pitchfork.com'\n",
    "            link_list.append(base_link + link['href'])\n",
    "        counter += 1\n",
    "    \n",
    "    return link_list\n",
    "\n",
    "def get_album_data(urls):\n",
    "    \n",
    "    album_df = pd.DataFrame({'publication_date': [], 'author': [], 'artist':[], 'album': [], 'score':[], 'review': []})\n",
    "    \n",
    "    counter = 1\n",
    "    for url in urls:\n",
    "        # Read in HTML from link\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        if soup.findAll(['div'], attrs={'class': ['contents dropcap']}):\n",
    "            review = soup.findAll(['div'], attrs={'class': ['contents dropcap']})\n",
    "\n",
    "        else:\n",
    "            review = soup.findAll(['div'], attrs={'class': ['review-detail__article-content']})\n",
    "        try:\n",
    "            x=soup.findAll(['script'], attrs={'type': [\"text/javascript\"]})\n",
    "            x = x[2].string\n",
    "            data = x.split(\"window.digitalData=\", 1)[1]\n",
    "            data = json.loads(data)\n",
    "\n",
    "            publication_date = pd.to_datetime(data['publishDate'])\n",
    "            author = data['authors']\n",
    "            artist = data['display'].split(':')[0].rstrip()\n",
    "            artist = artist.replace('&amp;', 'and')\n",
    "            artist = artist.replace('&quot;', '')\n",
    "            artist = artist.replace('-&gt;', '')\n",
    "            album = data['display'].split(':')[1].lstrip()\n",
    "            album = album.replace('&amp;', 'and')\n",
    "            album = album.replace('&quot;', '')\n",
    "            album = album.replace('-&gt;', '')\n",
    "\n",
    "            score = soup(text=re.compile('window.App'))[0]\n",
    "            score = score.split(\"window.App=\")[1].rstrip(';')\n",
    "            score = json.loads(score)\n",
    "            score=score['context']['dispatcher']['stores']['ReviewsStore']['items']\n",
    "            key = [i for i in score][0]\n",
    "            score = score[key]['tombstone']['albums'][0]['rating']['rating']\n",
    "\n",
    "            print('Artist: {}, Album: {}'.format(artist,album))\n",
    "\n",
    "            df_to_append = pd.DataFrame({'publication_date':[ publication_date], 'author': [author], 'artist':[artist], 'album': [album], 'score':[score], 'review': [review[0].text]})\n",
    "\n",
    "            album_df = album_df.append(df_to_append, ignore_index=True)\n",
    "            print('{} of {} completed'.format(counter,len(urls)))\n",
    "            counter += 1\n",
    "        except:\n",
    "            print('Could not extract {}'.format(url))\n",
    "            counter +=1\n",
    "    \n",
    "    album_df['score'] = album_df['score'].astype(float)\n",
    "    \n",
    "    return album_df\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    # lemmatize\n",
    "    #tokens = [i.lemma_ for i in sample]\n",
    "    #tokens = [i for i in tokens if i not in stopwords]\n",
    "    #tokens = [i for i in tokens if i != '-PRON-']\n",
    "    #tokens = [i for i in tokens if i.pos_ != 'SYM']\n",
    "    #tokens = [i for i in tokens if i.pos_ != 'PUNCT']\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    tokens = [i.lower_ for i in sample if i.lower_ not in list(stopwords) and i.pos_ != '-PRON-' and i.pos_ != 'SYM' and i.pos_ != \"PUNCT\"]\n",
    "    tokens = [i for i in tokens if i not in ['n’t','\"', ',', ',', ':', '.', '/', '-', '’s', '\\n', '—', '’', '’s']]\n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    #tokens = [i for i in freq_list if i in tokens]\n",
    "    \n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "    \n",
    "    doc = ' '.join(tokens)\n",
    "\n",
    "    return doc\n",
    "\n",
    "def create_corpus(df):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc_list = list(df['review'])\n",
    "    doc_list_new = []\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        try:\n",
    "            doc = nlp(doc)\n",
    "            doc = tokenizeText(doc)\n",
    "            doc_list_new.append(doc)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return doc_list_new\n",
    "\n",
    "def split_corpus(corpus, num_words):\n",
    "    word_list = []\n",
    "    for doc in corpus:\n",
    "        doc = doc.split(' ')\n",
    "        for word in doc:\n",
    "            word_list.append(word)\n",
    "    word_freq = Counter(word_list)\n",
    "    common_words = word_freq.most_common(num_words)\n",
    "    \n",
    "    most_freq_list = []\n",
    "    for i in common_words:\n",
    "        most_freq_list.append(i[0])\n",
    "    \n",
    "    return most_freq_list\n",
    "\n",
    "def get_best_new_music(num_urls):\n",
    "    album_df = pd.DataFrame({'artist':[], 'album': []})\n",
    "\n",
    "    for i in range(1,num_urls): \n",
    "        url = 'https://pitchfork.com/reviews/best/albums/?page=' + str(i)\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        soup = soup.findAll(['script'])\n",
    "        soup=soup[8].string.split(\"window.App=\")[1]\n",
    "        soup = soup.rstrip(';')\n",
    "        soup = json.loads(soup)\n",
    "        data = soup['context']['dispatcher']['stores']['ReviewsStore']['items']\n",
    "        id_list = list(data)\n",
    "        for i in id_list:\n",
    "            #print(i)\n",
    "            if data[i]['tombstone']['albums'][0]['album']['display_name']:\n",
    "                artist = data[i]['tombstone']['albums'][0]['album']['display_name']\n",
    "            else:\n",
    "                artist = data[i]['tombstone']['albums'][0]['album']['artists'][0]['display_name']\n",
    "            \n",
    "            artist = artist.replace('&amp;', 'and')\n",
    "            artist = artist.replace('&quot;', '')\n",
    "            artist = artist.replace('-&gt;', '')\n",
    "            \n",
    "            album = data[i]['tombstone']['albums'][0]['album']['display_name']\n",
    "            album = album.replace('&amp;', 'and')\n",
    "            album = album.replace('&quot;', '')\n",
    "            album = album.replace('-&gt;', '')\n",
    "            \n",
    "            df_to_append = pd.DataFrame({'artist':[artist], 'album': [album]})\n",
    "            album_df = album_df.append(df_to_append,ignore_index=True)\n",
    "            \n",
    "    return album_df\n",
    "\n",
    "def filter_by_frequency(zipped_items, top_items):\n",
    "    zip_list = []\n",
    "    counter = 0\n",
    "    for i in zipped_items:\n",
    "        if counter == top_items:\n",
    "            break\n",
    "        else:\n",
    "            print(i[0])\n",
    "            zip_list.append(i[0])\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "    return zip_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data if it hasn't been retrieved already\n",
    "This process takes a while, best to be done while you're gone at work or asleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are currently 59 pages of best new music.\n",
    "# Data is output as a df.\n",
    "best_new_music = get_best_new_music(59)\n",
    "best_new['category'] = 1\n",
    "\n",
    "# Output the data to a csv\n",
    "best_new_music.to_csv('best_new_music.csv', index=False)\n",
    "\n",
    "# Output the data to a more performance format, like Apache Feather\n",
    "feather.write_dataframe(best_new_music, 'best_new_music.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to scrape al lthe other data.\n",
    "# There are ~22000K reviews, it takes some time.\n",
    "# Data is output as a df.\n",
    "url_list = generate_urls('https://pitchfork.com/reviews/albums/?page=', 1703)\n",
    "urls = generate_artist_album_data(url_list)\n",
    "album_df = get_album_data(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the raw data to a csv and feather\n",
    "album_df.to_csv('pitchfork_reviews.csv')\n",
    "feather.write_dataframe(album_df,'pitchfork_reviews.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data if it's already been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchfork_data = feather.read_dataframe('pitchfork_reviews.feather')\n",
    "pitchfork_data.rename({'artist': 'artist_name'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a corpus of data using the reviews.\n",
    "This is accomplished using mainly the `spaCy` library, with some help from `nltk` to do PorterStemming instead of default lemmatization that spacy does. Lemmatization tends to lead to even more sparse matrices, which could be good (more data!) or bad (resource intensive for anything Feature Engineering or ML oriented).\n",
    "\n",
    "I have a set of helper function that cleans up all text data, removes symbols, stop words, and every other random quirk I found doing this. This part alone took a significant part of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(pitchfork_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit to do TFIDF vectorization. You can swap this out with simple counts\n",
    "# by using CountVectorizer().\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "matrix = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequencies in a list of tuples. Use a helper function\n",
    "# to retrieve the top n (here, 5000) words. \n",
    "\n",
    "# The rationale here is mainly that the data can get very large (20000 columns or so),\n",
    "# so it's good to only take what you need.\n",
    "\n",
    "zipped_frequency_list = sorted(zip(cv.get_feature_names(),\n",
    "    np.asarray(matrix.sum(axis=0)).ravel()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "vocab_list = filter_by_frequency(zipped_frequency_list, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit our data, but this time only with the list of top n words we chose.\n",
    "cv = TfidfVectorizer(vocabulary=vocab_list)\n",
    "matrix = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat matrices to gether to get a neater df. This step isn't necessary,\n",
    "# but I find dfs easier to examine the data with. Numpy arrays are more resource\n",
    "# efficient so this step isn't necessary.\n",
    "album_df2 = pd.concat([pitchfork_data, pd.DataFrame(matrix.todense(), columns=cv.get_feature_names())], axis=1).ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a bunch of stuff to merge best new music to review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "best_new = feather.read_dataframe('best_new_music.feather')\n",
    "best_new.rename({'artist':'artist_name'}, axis=1,inplace=True)\n",
    "album_df2=album_df2.merge(best_new, how='left', on='artist_name')\n",
    "album_df2[['category']] = album_df2[['category']].fillna(value=0)\n",
    "album_df2 = album_df2.clean_names()\n",
    "album_df2.drop('album_y', axis=1,inplace=True)\n",
    "feather.write_dataframe(album_df2, 'pitchfork_tfidf.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
